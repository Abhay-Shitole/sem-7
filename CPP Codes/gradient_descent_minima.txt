Gradient Descent to Find a Local Minimum
=========================================

Problem
-------
Implement gradient descent to find the local minimum of a function.
Example: Find the local minimum of y = (x + 3)^2 starting from x = 2.

Intuition & Theory
------------------
Gradient Descent (GD) is an iterative optimization algorithm used to find a local minimum of a differentiable function.
- Idea: Move in the opposite direction of the gradient (the slope), because that is the direction of steepest decrease.
- Update Rule (for one variable x):
    x_{t+1} = x_t - alpha * (dy/dx at x_t)
  where:
    - x_t is the current point,
    - alpha > 0 is the learning rate (step size),
    - dy/dx is the derivative of the objective function.

Convergence Considerations
--------------------------
- Learning rate (alpha): Too large can overshoot or diverge; too small makes progress very slow.
- Convexity: For convex functions, GD converges to the global minimum. For non-convex functions, GD may converge to a local minimum.
- Stopping criteria: Stop when the gradient magnitude is very small, or when changes in x (or y) are below a tolerance, or after a max number of iterations.

Applying GD to y = (x + 3)^2
-----------------------------
- Objective: y(x) = (x + 3)^2
- Derivative: dy/dx = 2 * (x + 3)
- True minimum: Occurs where dy/dx = 0 => x* = -3 (and y* = 0). This function is convex (a parabola), so the minimum is global.
- Starting point: x_0 = 2
- GD update: x_{t+1} = x_t - alpha * 2 * (x_t + 3)

Well-Chosen Defaults
--------------------
- Use alpha = 0.1 (safe and illustrative for this quadratic).
- Use tolerance = 1e-8 and max_iters = 10_000 to ensure convergence in practice.

Python Implementation
---------------------
The code below implements gradient descent for a 1D function and applies it to y = (x + 3)^2 starting at x = 2.

```python
def gradient_descent_1d(grad, x0, alpha=0.1, tol=1e-8, max_iters=10_000, verbose=True):
    """
    Performs gradient descent on a scalar function given its gradient.

    Parameters:
        grad      : callable, gradient function g(x) = dy/dx
        x0        : float, starting point
        alpha     : float, learning rate (step size)
        tol       : float, tolerance for stopping based on gradient magnitude
        max_iters : int, maximum number of iterations
        verbose   : bool, if True prints progress every few steps

    Returns:
        x         : float, the estimated minimizer
        history   : list of tuples (iter_idx, x, grad_x, y) for inspection
    """
    def y(x):  # Optional: the objective (for logging)
        return (x + 3)**2

    x = float(x0)
    history = []
    for t in range(1, max_iters + 1):
        g = grad(x)
        y_val = y(x)
        history.append((t, x, g, y_val))

        # Stopping when gradient is very small
        if abs(g) < tol:
            if verbose:
                print(f"Stopped at iter {t}: |grad| < tol. x={x:.12f}, y={y_val:.12e}, grad={g:.3e}")
            break

        # Update step
        x = x - alpha * g

        # Optionally show progress
        if verbose and (t <= 10 or t % 100 == 0):
            print(f"iter={t:5d}, x={x:.6f}, y={(x+3)**2:.6e}, grad={grad(x):.6e}")

    return x, history


# Problem-specific gradient for y = (x + 3)^2
def grad_y(x):
    return 2 * (x + 3)


if __name__ == "__main__":
    x0 = 2.0          # starting point
    alpha = 0.1       # learning rate
    tol = 1e-8
    max_iters = 10000

    xmin, hist = gradient_descent_1d(grad_y, x0, alpha, tol, max_iters, verbose=True)
    print("\nEstimated minimum at x ≈", xmin)
    print("Function value at minimum y(x) ≈", (xmin + 3)**2)
    print("(True minimum is at x = -3, y = 0)")
```

What You Should Observe
-----------------------
- Starting from x=2, successive updates move x toward -3.
- Because the function is a simple convex quadratic, convergence is fast and stable for alpha=0.1.
- If you increase alpha too much (e.g., alpha=1.2), the method can oscillate or diverge.

Hand-Checked First Iterations (alpha = 0.1)
-------------------------------------------
- x_0 = 2
- grad(x_0) = 2*(2+3) = 10
- x_1 = 2 - 0.1*10 = 1.0
- grad(x_1) = 2*(1+3) = 8
- x_2 = 1 - 0.1*8 = 0.2
- x_3 = 0.2 - 0.1*2*(0.2+3) = 0.2 - 0.1*6.4 = -0.44
- ...
The sequence converges to x* = -3 and y* = 0.

Key Takeaways
-------------
1) Gradient descent moves opposite the gradient to reduce the function value.
2) Learning rate selection is crucial: too small = slow; too large = unstable.
3) For convex quadratics like (x+3)^2, GD converges to the unique global minimum at x = -3.
